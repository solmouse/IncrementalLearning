import tensorflow as tf
import numpy as np
import tensorflow_datasets as tfds
import time

IMG_SIZE = 224
INIT_CLASSES = 2
SAVED_MODEL_DIR = ""
BATCH_SIZE = 32
data_dir = ""

# CPUìš©
NUM_TRAIN_SAMPLES = 500
NUM_EPOCHS = 2

# -------------------------------
# MobileNetV2 ë²„ë¦¬ê³  ë‹¨ìˆœ CNN ì‚¬ìš© (ê³µì‹ ì˜ˆì œ ë°©ì‹)
# -------------------------------
class Model(tf.Module):
    def __init__(self, num_classes=INIT_CLASSES):
        super().__init__()
        
        # ğŸ’¡ ë‹¨ìˆœí•˜ì§€ë§Œ ê°•ë ¥í•œ CNN
        self.model = tf.keras.Sequential([
            # ì…ë ¥: (224, 224, 3)
            tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),
            tf.keras.layers.MaxPooling2D(2),
            # (111, 111, 32)
            
            tf.keras.layers.Conv2D(64, 3, activation='relu'),
            tf.keras.layers.MaxPooling2D(2),
            # (54, 54, 64)
            
            tf.keras.layers.Conv2D(128, 3, activation='relu'),
            tf.keras.layers.MaxPooling2D(2),
            # (26, 26, 128)
            
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(128, activation='relu', name='dense_1'),
            tf.keras.layers.Dense(num_classes, name='dense_2')
        ])
        
        self.model.compile(
            optimizer='sgd',
            loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True)
        )
    
    @tf.function(input_signature=[
    tf.TensorSpec([None, IMG_SIZE, IMG_SIZE, 3], tf.float32),
    tf.TensorSpec([None, None], tf.float32)
    ])
    def train(self, x, y):
        with tf.GradientTape() as tape:
            logits = self.model(x, training=True)
            loss = self.model.loss(y, logits)
        
        grads = tape.gradient(loss, self.model.trainable_variables)
        lr = 0.001
        # optimizer.apply_gradients ëŒ€ì‹  ì§ì ‘ ì—…ë°ì´íŠ¸
        for w, g in zip(self.model.trainable_variables, grads):
            w.assign_sub(lr * g)

        return {"loss": loss}


    @tf.function(input_signature=[
        tf.TensorSpec([None, IMG_SIZE, IMG_SIZE, 3], tf.float32)
    ])
    def infer(self, x):
        logits = self.model(x, training=False)
        probabilities = tf.nn.softmax(logits, axis=-1)
        return {
            "output": probabilities,
            "logits": logits
        }
    
    @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])
    def save(self, checkpoint_path):
        tensor_names = []
        tensors_to_save = []
        for layer in self.model.layers:
            for w in layer.weights:
                # ê³ ìœ  ì´ë¦„ ë¶€ì—¬
                unique_name = layer.name + "/" + w.name
                tensor_names.append(unique_name)
                tensors_to_save.append(w)

        tf.raw_ops.Save(
            filename=checkpoint_path,
            tensor_names=tensor_names,
            data=tensors_to_save,
            name='save'
        )
        return {"checkpoint_path": checkpoint_path}

    @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])
    def restore(self, checkpoint_path):
        restored_tensors = {}
        for layer in self.model.layers:
            for w in layer.weights:
                unique_name = layer.name + "/" + w.name
                restored = tf.raw_ops.Restore(
                    file_pattern=checkpoint_path,
                    tensor_name=unique_name,
                    dt=w.dtype,
                    name='restore'
                )
                restored.set_shape(w.shape)
                w.assign(restored)
                restored_tensors[unique_name] = restored
        return restored_tensors

# -------------------------------
# 1. ëª¨ë¸ ìƒì„± ë° ë³€ìˆ˜ í™•ì¸
# -------------------------------
m = Model(num_classes=INIT_CLASSES)

print("="*60)
print("ëª¨ë¸ êµ¬ì¡°:")
m.model.summary()
print(f"\nì´ íŒŒë¼ë¯¸í„°: {m.model.count_params():,}")
print(f"í•™ìŠµ ê°€ëŠ¥í•œ ë³€ìˆ˜ ìˆ˜: {len(m.model.trainable_variables)}")
print("="*60)

# -------------------------------
# 2. ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬
# -------------------------------
def preprocess(image, label):
    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))
    image = tf.cast(image, tf.float32) / 255.0
    label = tf.one_hot(label, INIT_CLASSES)
    return image, label

print("\në°ì´í„°ì…‹ ë¡œë”© ì¤‘...")
train_dataset, test_dataset = tfds.load(
    "cats_vs_dogs",
    split=["train[:80%]", "train[80%:]"],
    as_supervised=True,
    data_dir=data_dir
)

train_ds = (train_dataset
    .take(NUM_TRAIN_SAMPLES)
    .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)
    .cache()
    .shuffle(500)
    .batch(BATCH_SIZE)
    .prefetch(tf.data.AUTOTUNE)
)

test_ds = (test_dataset
    .take(100)
    .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)
    .cache()
    .batch(BATCH_SIZE)
    .prefetch(tf.data.AUTOTUNE)
)

for images, labels in train_ds.take(1):
    print(f"ì´ë¯¸ì§€ ë°°ì¹˜ shape: {images.shape}")
    print(f"ë¼ë²¨ ë°°ì¹˜ shape: {labels.shape}")

# -------------------------------
# 3. ì´ˆê¸° í•™ìŠµ
# -------------------------------
print("\nì´ˆê¸° í•™ìŠµ ì‹œì‘...")

for epoch in range(NUM_EPOCHS):
    losses = []
    batch_count = 0
    start_time = time.time()
    
    for batch in train_ds:
        x, y = batch
        result = m.train(x, y)
        losses.append(result['loss'].numpy())
        batch_count += 1
        
        if batch_count % 5 == 0:
            print(f"  Batch {batch_count}/{NUM_TRAIN_SAMPLES//BATCH_SIZE}, Loss: {result['loss'].numpy():.4f}")
    
    elapsed = time.time() - start_time
    print(f"âœ… Epoch {epoch+1}/{NUM_EPOCHS} ì™„ë£Œ - í‰ê·  Loss: {np.mean(losses):.4f}, ì†Œìš”ì‹œê°„: {elapsed:.1f}ì´ˆ")

# ì²´í¬í¬ì¸íŠ¸ ì €ì¥
checkpoint_path = 'D:/2025-1/friday/last/checkpoint.ckpt'
m.save(checkpoint_path=np.array(checkpoint_path, dtype=np.string_))
print(f"\nğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}")

# -------------------------------
# 4. SavedModel ì €ì¥
# -------------------------------
print(f"\nğŸ“¦ SavedModel ì €ì¥ ì¤‘...")
tf.saved_model.save(
    m,
    SAVED_MODEL_DIR,
    signatures={
        'train': m.train.get_concrete_function(),
        'infer': m.infer.get_concrete_function(),
        'save': m.save.get_concrete_function(),
        'restore': m.restore.get_concrete_function(),
    }
)
print(f"âœ… SavedModel ì €ì¥ ì™„ë£Œ: {SAVED_MODEL_DIR}")

# -------------------------------
# 5. TFLite ë³€í™˜
# -------------------------------
print("\nğŸ”„ TFLite ë³€í™˜ ì¤‘...")
converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,
    tf.lite.OpsSet.SELECT_TF_OPS
]
converter.experimental_enable_resource_variables = True

tflite_model = converter.convert()

tflite_path = "model.tflite"
with open(tflite_path, 'wb') as f:
    f.write(tflite_model)
print(f"âœ… TFLite ì €ì¥ ì™„ë£Œ: {tflite_path} ({len(tflite_model)/1024/1024:.2f} MB)")

# -------------------------------
# 6. TFLite ì¦ë¶„í•™ìŠµ í…ŒìŠ¤íŠ¸
# -------------------------------
print("\n" + "="*60)
print("ğŸš€ TFLite ì¦ë¶„ í•™ìŠµ í…ŒìŠ¤íŠ¸")
print("="*60)

interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()

# ëª¨ë“  í…ì„œ í™•ì¸
for detail in interpreter.get_tensor_details():
    print(f"ì´ë¦„: {detail['name']}")
    print(f"  dtype: {detail['dtype']}, shape: {detail['shape']}")
    print(f"  index: {detail['index']}")
    print()

print("\nì‚¬ìš© ê°€ëŠ¥í•œ ì‹œê·¸ë‹ˆì²˜:")
for sig in interpreter.get_signature_list():
    print(f"  âœ“ {sig}")

train_fn = interpreter.get_signature_runner("train")
infer_fn = interpreter.get_signature_runner("infer")
save_fn = interpreter.get_signature_runner("save")
restore_fn = interpreter.get_signature_runner("restore")

# ì²´í¬í¬ì¸íŠ¸ ë³µì›
print("\nğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë³µì› ì¤‘...")
restore_fn(checkpoint_path=np.array(checkpoint_path, dtype=np.string_))
print("âœ… ì²´í¬í¬ì¸íŠ¸ ë³µì› ì™„ë£Œ!")

# ì¦ë¶„ í•™ìŠµ
print("\nğŸ¯ ì¦ë¶„ í•™ìŠµ ì‹œì‘...")
step_count = 0
for batch in train_ds.take(10):
    new_x, new_y = batch
    try:
        result = train_fn(x=new_x.numpy(), y=new_y.numpy())
        loss_val = float(result['loss'])   # âœ… ì•ˆì „í•˜ê²Œ ë³€í™˜
        step_count += 1
        print(f"  âœ“ Step {step_count}, Loss: {loss_val:.4f}")
    except Exception as e:
        print(f"  âœ— Step {step_count+1} ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        break

if step_count > 0:
    # ì—…ë°ì´íŠ¸ëœ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    new_checkpoint = 'D:/2025-1/friday/last/checkpoint_updated.ckpt'
    print(f"\nğŸ’¾ ì—…ë°ì´íŠ¸ëœ ê°€ì¤‘ì¹˜ ì €ì¥: {new_checkpoint}")
    save_fn(checkpoint_path=np.array(new_checkpoint, dtype=np.string_))


# ì¶”ë¡  í…ŒìŠ¤íŠ¸
print("\nğŸ”® ì¶”ë¡  í…ŒìŠ¤íŠ¸...")
for test_batch in test_ds.take(1):
    test_x, test_y = test_batch
    result = infer_fn(x=test_x.numpy())
    predictions = np.argmax(result['output'], axis=1)
    true_labels = np.argmax(test_y.numpy(), axis=1)
    
    accuracy = np.mean(predictions == true_labels)
    print(f"  ì˜ˆì¸¡ shape: {result['output'].shape}")
    print(f"  ì˜ˆì¸¡ í´ë˜ìŠ¤: {predictions[:10]}")
    print(f"  ì‹¤ì œ í´ë˜ìŠ¤: {true_labels[:10]}")
    print(f"  âœ… ì •í™•ë„: {accuracy:.2%}")


print("\n" + "="*60)
print("ğŸ‰ ì™„ë£Œ! TFLite ì˜¨ë””ë°”ì´ìŠ¤ í•™ìŠµ ì„±ê³µ!")
print("="*60)

# -------------------------------
# 7. ì—…ë°ì´íŠ¸ ì „/í›„ ë¹„êµ
# -------------------------------
print("\n" + "="*60)
print("ğŸ“Š ì—…ë°ì´íŠ¸ ì „/í›„ ì„±ëŠ¥ ë¹„êµ")
print("="*60)

def evaluate_model(interpreter, checkpoint_path, test_ds):
    """íŠ¹ì • ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì™€ì„œ test accuracy í‰ê°€"""
    infer_fn = interpreter.get_signature_runner("infer")
    restore_fn = interpreter.get_signature_runner("restore")
    restore_fn(checkpoint_path=np.array(checkpoint_path, dtype=np.string_))

    total, correct = 0, 0
    for test_x, test_y in test_ds:
        result = infer_fn(x=test_x.numpy())
        preds = np.argmax(result["output"], axis=1)
        labels = np.argmax(test_y.numpy(), axis=1)
        correct += np.sum(preds == labels)
        total += len(labels)
    return correct / total

# ì—…ë°ì´íŠ¸ ì „/í›„ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
ckpt_before = "D:/2025-1/friday/last/checkpoint.ckpt"
ckpt_after  = "D:/2025-1/friday/last/checkpoint_updated.ckpt"

# ìƒˆë¡œìš´ ì¸í„°í”„ë¦¬í„° ë‘ ê°œ ìƒì„± (ê°ê° ë”°ë¡œ restore í•´ì•¼ ì•ˆì „)
interpreter_before = tf.lite.Interpreter(model_path="model.tflite")
interpreter_before.allocate_tensors()

interpreter_after = tf.lite.Interpreter(model_path="model.tflite")
interpreter_after.allocate_tensors()

# í‰ê°€
acc_before = evaluate_model(interpreter_before, ckpt_before, test_ds)
acc_after = evaluate_model(interpreter_after, ckpt_after, test_ds)

print(f"ğŸ”¹ ì—…ë°ì´íŠ¸ ì „ ì •í™•ë„: {acc_before:.2%}")
print(f"ğŸ”¹ ì—…ë°ì´íŠ¸ í›„ ì •í™•ë„: {acc_after:.2%}")
print("="*60)


